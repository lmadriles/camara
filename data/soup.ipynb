{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "\n",
    "Este notebook é uma demonstração de como extrair dados de votação de algum evento parlamentar da Câmara dos Deputados.\n",
    "\n",
    "Você pode utulizá-lo para extrair dados das votações que seguem o mesmo padrão html da utilizada nesse exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentação das bibliotecas utilizadas: \n",
    "\n",
    "<a href=\"https://docs.python-requests.org/en/latest/\" title=\"Requests\">Requests</a>\n",
    "\n",
    "<a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/#\" title=\"BeautifulSoup\">BeautifulSoup</a>\n",
    "\n",
    "<a href=\"https://pandas.pydata.org/docs/\" title=\"Pandas\">Pandas</a>\n",
    "\n",
    "<a href=\"https://docs.python.org/3/library/re.html\" title=\"Regular Expressions\">Regular Expressions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permissões:\n",
    "\n",
    "Caso use esse algoritmo para raspagem, não utilizem urls que violem o <a href=\"https://www.camara.leg.br/robots.txt\" title=\"Protocolo de Exclusão de Robôs\">Protocolo de Exclusão de Robôs</a>:\n",
    "\n",
    "- User-Agent: * \n",
    "- Disallow: /sileg/prop_lista* \n",
    "- Disallow: /internet/sileg/prop_lista* \n",
    "- Disallow: /sileg/Prop_lista* \n",
    "- Disallow: /internet/sileg/Prop_lista* \n",
    "- Disallow: /*arvore-de-apensados \n",
    "- Disallow: /proposicoesWeb/prop_arvore_tramitacoes \n",
    "- Disallow: /sileg/prop_arvore_tramitacoes \n",
    "- Disallow: /internet/deputado/Dep_Lista* \n",
    "- Disallow: /transparencia/recursos-humanos/remuneracao/*\n",
    "- Disallow: /transparencia/recursos-humanos/contratos-terceirizacao* \n",
    "- Disallow: /transparencia/recursos-humanos/funcionarios* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as bibliotecas necessárias\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re # expressões regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_html(url):\n",
    "    \"\"\"Extract the html code of the page.\n",
    "    \n",
    "    Args:\n",
    "        url (str): page's url\n",
    "\n",
    "    Returns:\n",
    "        html_str (str): html extracted\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url) \n",
    "        html_str = r.text \n",
    "    except:\n",
    "        raise HTTPException(404)\n",
    "        \n",
    "    return html_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo temos um exemplo de um elemento da lista que contém todas as informações que precisamos por palarmentar.\n",
    "\n",
    "    <li>\n",
    "      <span class=\"nome\">Abou Anni</span>\n",
    "      <span class=\"nomePartido\">(PSL-SP)</span>\n",
    "      <span class=\"votou\">-votou<span class=\"voto sim\">Sim</span></span>\n",
    "     </li>\n",
    "\n",
    "Precisamos navegar até o elemento de cada deputado e extrair as informações relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(html_str):\n",
    "    \"\"\"Extract, assemble and returns the voting information from html in a 2D list.\n",
    "    \n",
    "    Args:\n",
    "        html_str (str): html as a string\n",
    "    \n",
    "    Returns:\n",
    "        vote_info (list): data scraped from html with shape (number of votants, number of features).\n",
    "    \n",
    "    \"\"\"\n",
    "    # crinado o objeto soup\n",
    "    soup = BeautifulSoup(html_str) \n",
    "\n",
    "    # visualizando o objeto soup:\n",
    "    # print(soup.prettify()) # com identação\n",
    "    \n",
    "    # inicializa a lista\n",
    "    vote_info = []\n",
    "    \n",
    "    # percorre o item de lista com a info:\n",
    "    for elemento in soup.find_all('span', attrs={\"class\":\"nome\"}):\n",
    "        lista = []\n",
    "        lista.append(elemento.parent.contents[1].string) # nome do parlamentar\n",
    "        \n",
    "        lista.append(elemento.parent.contents[3].string) # partido e estado\n",
    "        \n",
    "        \n",
    "        # Voto\n",
    "        if not elemento.parent.select('span.votou span'): # lista vazia não tem método .string:\n",
    "            lista.append('Sem voto') # ausência\n",
    "        else:\n",
    "            lista.append(elemento.parent.select('span.votou span')[0].string) # voto\n",
    "        vote_info.append(lista)\n",
    "    return vote_info\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(vote_info, csv_name='saved'):\n",
    "    \"\"\"\n",
    "    Convert 2D list into pandas DataFrame and store it into a .csv file.\n",
    "    \n",
    "    Args:\n",
    "        vote_info (list): 2D list with the information gathered\n",
    "        csv_name (str): name of the file to be created\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(vote_info, columns=['nome', 'partido', 'voto'])\n",
    "    csv_name = csv_name + '.csv'\n",
    "    df.to_csv(csv_name, index=False)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url, csv_name):\n",
    "    \"\"\"\n",
    "    Recieves a url, scrape the voting information and saves it into a csv file.\n",
    "    \n",
    "    Args:\n",
    "        url (str): page's url\n",
    "        csv_name (str): name of the file to be created\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    html = extract_html(url)\n",
    "    vote_info = scrape_page(html)\n",
    "    save_data(vote_info, csv_name)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    url = 'https://www.camara.leg.br/presenca-comissoes/votacao-portal?reuniao=62583&itemVotacao=9968&ordenacao=Nome'\n",
    "    scraper(url, 'pec135')\n",
    "    \n",
    "    df = pd.read_csv('pec135.csv')\n",
    "    \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
